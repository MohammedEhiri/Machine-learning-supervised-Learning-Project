{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9241e8-d157-4ce5-a56f-d0257c876928",
   "metadata": {},
   "source": [
    "<h5 style=\"text-align:right;color:gray;\">Réalisé par: <span style=\"color:orange;font-weight:bold;\">Ehiri Mohammed</span></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265828f9-732a-4697-a919-e412f0f93270",
   "metadata": {},
   "source": [
    "# ***`Partie 1/2 : La regression lineaire`***\n",
    "\n",
    "\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7269eb4-b351-46c7-8531-d02509b22aeb",
   "metadata": {},
   "source": [
    "# \n",
    "**Importation des bibliotheques necessaires**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2550a4-2dac-4b18-8fc0-a863a2693eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7160d-40e4-458b-9789-7ca58bb0b33d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "**Chargement des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ef625-1e3c-4026-9d74-bdf833594e62",
   "metadata": {},
   "source": [
    "- Nous utilisons la librairie **pandas** de Python pour présenter les données en format **DataFrame**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899806aa-1c76-4a73-adc8-bc77ff8401bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/regresson.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3ee66-396e-44f3-8489-c9160750ac7e",
   "metadata": {},
   "source": [
    "# \n",
    "**Phase d'Analyse des données**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7d243-6271-4593-8278-2946dde87481",
   "metadata": {},
   "source": [
    "- La méthode `head()` permet d'afficher 5 premières lignes du dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cf431-983a-4a12-9ee2-a22b06857616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d25ba-37a9-493a-8b8a-1f45e228e483",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "- Affichage des informations concernant le type des colonnes par la methode  `df.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb782b58-bb43-4bbf-831d-ae5ac5a76544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88f7aa-dd22-45a6-84f2-9c16d420e2fb",
   "metadata": {},
   "source": [
    "# \n",
    "- La méthode **`describe`** affiche des statistiques descriptives des données numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a78e7-a7b2-44ea-b80f-12fd52f81132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c831e-9af5-48a1-bf66-7afc21dbad28",
   "metadata": {},
   "source": [
    "# \n",
    "- **`sns.heatmap()`** est une fonction de la bibliothèque Python Seaborn qui permet de créer des cartes de chaleur pour visualiser les données tabulaires sous forme de matrices. Cette fonction peut être utilisée pour explorer les relations entre deux variables, en affichant une couleur différente pour chaque valeur de la variable sur la carte de chaleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4a8df-1ede-4669-820b-c6d73ab279ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(), cmap=\"RdBu\")\n",
    "plt.title(\"Correlations Between Variables\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a7a6d-79c5-4774-856c-005b9a805765",
   "metadata": {},
   "source": [
    "**Selection des colonnes | variables qui nous interesse le plus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb52ab-ea59-45e8-a45a-618efa78e599",
   "metadata": {},
   "source": [
    "- nous allons choisir seulement les variables qui ont un indice de correlation `> 0.5` ou `< -0.5`  avec la variable cible  `SalePrice`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53329ce-60f9-4376-8e53-2a9e35752cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colonnes_numeriques = list( df.corr()[\"SalePrice\"][ ( df.corr()[\"SalePrice\"] > 0.50 ) | ( df.corr()[\"SalePrice\"] < -0.50 )].index)\n",
    "colonnes_qualitatives = [\"MSZoning\", \"Utilities\",\"BldgType\",\"Heating\",\"KitchenQual\",\"SaleCondition\",\"LandSlope\"]\n",
    "colonnes_importantes = colonnes_numeriques + colonnes_qualitatives\n",
    "\n",
    "df = df[colonnes_importantes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a35f58-7598-4caa-a451-99fed041e8e6",
   "metadata": {},
   "source": [
    "# \n",
    "- **`df.isna().sum()`** est une méthode de pandas qui renvoie un objet Series contenant le nombre de valeurs manquantes (NaN) pour chaque colonne d'un DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7895c7-4de5-4571-ac39-0794e08c3de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(\"Total des valeurs monquantes:\",df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222a659-1612-43c0-a97e-924338650273",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "- **`sns.pairplot()`** est une fonction de la bibliothèque Python Seaborn qui permet de créer des graphiques de dispersion (scatter plots) et des histogrammes pour visualiser les relations entre paires de variables dans un ensemble de données. Chaque paire de variables est représentée sur un graphique 2D, avec les valeurs d'une variable sur l'axe des abscisses et les valeurs de l'autre variable sur l'axe des ordonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc101be9-9953-46b7-9278-0bb9ea12c9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[colonnes_numeriques]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f79a4-d382-4518-9968-e1f2c4742b32",
   "metadata": {},
   "source": [
    "# \n",
    "**Preparation des données**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e559bb-1c26-4e42-882c-772affb6d6cc",
   "metadata": {},
   "source": [
    "##### Decoupage du jeu de données en `X` et `y`:\n",
    "- X : les variables explicatives (Explained variables)\n",
    "- y : la variable à expliquer ou la variable cible (Target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af263e0b-9733-49c7-9b7d-1294af7e6b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"SalePrice\", axis=1)\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860d5a1-c0f0-4d58-a18b-18bbf4afe21f",
   "metadata": {},
   "source": [
    "# \n",
    "**Decodage des variables qualitatives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5deac7-43e1-4f20-8374-948d6394c500",
   "metadata": {},
   "source": [
    "- La fonction pd.get_dummies() est une fonction de la bibliothèque Python Pandas qui permet de créer des variables indicatrices (ou variables dummy) à partir d'une variable catégorielle (ou nominale). Elle transforme une variable catégorielle en plusieurs variables binaires (0 ou 1), chacune représentant une catégorie de la variable d'origine. Cela est souvent utile pour l'analyse de données et la modélisation, car les modèles de machine learning ne peuvent pas directement traiter les variables catégorielles. La fonction permet également de spécifier des préfixes pour les noms de colonnes des nouvelles variables créées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90945b-03dc-4c44-b611-b25e8d360cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=colonnes_qualitatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5baca-0f13-4630-9310-513cd1ff024a",
   "metadata": {},
   "source": [
    "# \n",
    "**Standardisation des données**\n",
    "\n",
    "\n",
    "- La standardisation des données en apprentissage automatique (ML) est une étape importante dans le prétraitement des données avant de les utiliser pour entraîner un modèle de ML. La standardisation des données vise à mettre toutes les variables d'entrée à la même échelle afin que les modèles de ML puissent fonctionner plus efficacement.\n",
    "\n",
    "- La standardisation des données implique de transformer les données d'entrée de manière à avoir une moyenne nulle et une variance unitaire. Cela peut être fait en soustrayant la moyenne de chaque variable d'entrée et en divisant par l'écart type. Cette opération est généralement effectuée sur chaque colonne de la matrice de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d1131-130d-4fc7-a9f6-3b6dc0c57bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "colonnes_numeriques.remove(\"SalePrice\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[colonnes_numeriques] = scaler.fit_transform(X[colonnes_numeriques])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2580ccc-72c5-442e-b3bf-4dfb1d90d29f",
   "metadata": {},
   "source": [
    "# \n",
    "**Division de données en une patie d'apprentissage et une autre de test**\n",
    "\n",
    "- La fonction **`train_test_split`** est une fonctionnalité courante dans le prétraitement des données pour les modèles de machine learning. Elle est utilisée pour séparer les données en un ensemble **d'apprentissage** et un ensemble de **test**, afin de pouvoir évaluer la performance du modèle sur des données qui n'ont pas été utilisées pour l'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a436e-458b-436d-9feb-3272f4ed650f",
   "metadata": {},
   "source": [
    "<img src=\"train_test_split.png\" alt=\"Splitting original dataset in training and test\" width=\"400\" aling=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa0556-c2b1-4cfe-9187-c58fb26c0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49f2a9-ffc0-4cd4-9aea-41ba582ac49b",
   "metadata": {},
   "source": [
    "#\n",
    "- Definition d'une fonction pour l'evaluation des modeles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b81411-a4e6-4c4f-b0b3-4f4e821ef118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "def evaluation(y, predictions):\n",
    "    \n",
    "    mae = mean_absolute_error(y, predictions)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y, predictions))\n",
    "    r2 = r2_score(y, predictions)\n",
    "    print(\"-+-\"*30)\n",
    "    print(\"L'evaluation du modele selon les metriques suivantes:\")\n",
    "    print(\"-+-\"*30)\n",
    "    print()\n",
    "    print(\"MAE  ===> \", mae)\n",
    "    print(\"MSE  ===> \", mse)\n",
    "    print(\"RMSE   ===> \", rmse)\n",
    "    print(\"R2 Score   ===> \", r2)\n",
    "    print(\"-+-\"*30)\n",
    "    \n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6606a-9617-42dc-aeec-f9adc7af7d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Coeff_intercept_model(model):\n",
    "    coefs = model.coef_\n",
    "    intercept = model.intercept_\n",
    "    \n",
    "    print(\"-+-\"*30)\n",
    "    print(\"Coefficients : \\n\", coefs)\n",
    "    print(\"-+-\"*30)\n",
    "    print(\"Biais : \", intercept)\n",
    "    print(\"-+-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121469b-6c42-41bb-977a-8ef3c856e3a9",
   "metadata": {},
   "source": [
    "- Cette fonction prend en entrée un modèle de régression linéaire entraîné avec scikit-learn et utilise les attributs `coef_` et `intercept_` pour afficher les coefficients et le biais respectivement.\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3db7cf-03f6-4e15-a4bd-b6d8a5f44e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = pd.DataFrame(columns=[\"Model\",\"MAE\",\"MSE\",\"RMSE\",\"R2 Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67526e1a-518b-4839-b5dc-901f04698adf",
   "metadata": {},
   "source": [
    "- La ligne de code crée un objet pandas DataFrame vide appelé \"models\" qui aura quatre colonnes: \"Model\", \"MAE\", \"MSE\", \"RMSE\", et \"R2 Score\". Cette ligne sert à stocker les résultats de l'évaluation des différents modèles de régression que nous allons entraîner, afin de pouvoir les comparer facilement et choisir le meilleur modèle en fonction des métriques d'évaluation. Chaque ligne de ce DataFrame correspondra à un modèle, et chaque colonne contiendra une métrique d'évaluation différente pour ce modèle.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ca5ca-4caf-4308-9424-11176a874508",
   "metadata": {},
   "source": [
    "# \n",
    "#### K-plus proches voisins pour la regression\n",
    "- L'algorithme **`KNeighborsRegressor`** fonctionne en trouvant les k points de données les plus proches du point de données que l'on souhaite prédire. Ensuite, il calcule la moyenne (ou la médiane) de la variable cible des k points les plus proches et utilise cette valeur pour prédire la variable cible du point de données en question.\n",
    "\n",
    "- La distance entre les points de données est généralement mesurée à l'aide de la distance euclidienne, bien qu'il existe d'autres mesures de distance possibles. Le paramètre **k**, qui représente le nombre de voisins à prendre en compte pour chaque prédiction, doit être défini par l'utilisateur.\n",
    "\n",
    "- `KNeighborsRegressor` est un algorithme simple et efficace pour la régression de données. Il peut être utile pour les ensembles de données de petite à moyenne taille et pour les problèmes de régression non linéaires. Cependant, il peut être moins efficace pour les grands ensembles de données en raison de la complexité de calcul de la distance entre chaque point de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199e987-e0a8-49cf-b598-de6454aa88c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 41)}\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=20)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "n_neighbors= grid_search.best_params_[\"n_neighbors\"]\n",
    "\n",
    "print(\"Meilleure valeur de n_neighbors : \", n_neighbors)\n",
    "\n",
    "knn_best = KNeighborsRegressor(n_neighbors=grid_search.best_params_[\"n_neighbors\"])\n",
    "knn_best.fit(X_train, y_train)\n",
    "y_pred = knn_best.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3221d6d-917c-48e4-81be-be8b8f5c48de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=n_neighbors) # n_neighbors = 8\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_model = knn.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, knn_model)\n",
    "new_row = {\"Model\": \"K-plus proches voisins\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10289aa6-bf59-4b79-8c64-de6d1e897495",
   "metadata": {},
   "source": [
    "# \n",
    "# **La Regression Lineaire**\n",
    "`LinearRegression` est une classe de la bibliothèque Python scikit-learn (sklearn) qui permet de réaliser une régression linéaire. La régression linéaire est une technique d'apprentissage supervisé qui vise à établir une relation linéaire entre une variable cible (aussi appelée variable dépendante) et une ou plusieurs variables prédictives (aussi appelées variables indépendantes).\n",
    "\n",
    "La classe `LinearRegression` implémente l'algorithme de la moindre carrée ordinaire pour estimer les coefficients de la régression linéaire. Cet algorithme minimise la somme des carrés des différences entre les valeurs prédites et les valeurs observées de la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619ca78-2f6b-4119-a66d-4cfcb56a4eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "lin_reg_model = lin_reg.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, lin_reg_model)\n",
    "\n",
    "new_row = {\"Model\": \"Regression Lineaire\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40eb156-c103-4dc4-8013-ad8b38b5b00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Coeff_intercept_model(lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1afc9-2c32-4203-b819-2a8ed2dbad47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, lin_reg_model)\n",
    "plt.xlabel('Valeurs réelles')\n",
    "plt.ylabel('Prédiction')\n",
    "plt.title('Prédiction vs. Valeurs réelles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94472290-a4dd-43aa-b576-9679a69e87fb",
   "metadata": {},
   "source": [
    "- Dans cet exemple, `lin_reg` est le modèle de régression linéaire entraîné sur les données d'entraînement, X_train et y_train. Les prédictions sont effectuées sur les données de test, X_test, et comparées aux vraies valeurs correspondantes, y_test. Le graphique résultant montre la relation entre les vraies valeurs et les prédictions, ce qui peut donner une idée de la qualité du modèle.\n",
    "\n",
    "- nous pouvons en deduire qu'il y a une correlation entre les prédictions et les vraies valeurs , ce qui signifie que le modele `lin_reg`  de régression linéaire donne de bons resultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b65960-1b5d-485e-9c86-d9079fad34d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "# Ridge & Lasso\n",
    "- **`Ridge`** est une technique de régularisation pour les modèles de régression linéaire. L'objectif de la régularisation est de prévenir le surapprentissage (overfitting) en ajoutant une pénalité aux coefficients du modèle afin de les rendre plus petits.\n",
    "\n",
    "La régression Ridge est une variante de la régression linéaire qui utilise une fonction de coût modifiée pour ajouter une pénalité à la somme des carrés des coefficients. La fonction de coût pour la régression Ridge est :\n",
    "\n",
    "**`J = MSE + alpha * sum(w^2)`**\n",
    "\n",
    "où `MSE` est l'erreur quadratique moyenne (mean squared error) de la régression linéaire ordinaire, `alpha` est un paramètre de régularisation (aussi appelé hyperparamètre) qui contrôle l'importance de la pénalité, `w` est le vecteur des coefficients du modèle (y compris l'interception), et `sum(w^2)` est la somme des carrés des coefficients.\n",
    "\n",
    "- La valeur de alpha doit être choisie de manière à trouver un compromis entre l'ajustement des données et la régularisation. Une valeur plus élevée de alpha entraîne une plus grande pénalité et des coefficients plus petits, ce qui peut réduire le surapprentissage mais également réduire les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79c6d0-b0e9-4c8d-b3f6-4b15970c2f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1, 10, 1e2, 1e3], cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Affichage de la meilleure valeur de alpha\n",
    "print(\"Meilleure valeur de alpha :\", ridge_cv.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b0920-9089-47fe-a4f7-b630828de6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=ridge_cv.alpha_)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "ridge_model = ridge.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, ridge_model)\n",
    "\n",
    "new_row = {\"Model\": \"ridge_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54082ea5-e6ce-439f-9b94-c013eb2168be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Affichage des coefficients et le biais\n",
    "Coeff_intercept_model(ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcea347-588f-41a3-99f0-c442754f1fb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Recapitulation**\n",
    "`Dans cet exemple, nous créons un nouvel objet Ridge avec la meilleure valeur de alpha trouvée, puis entraînons le modèle sur toutes les données d'entraînement (X_train et y_train). Nous pouvons ensuite évaluer les performances du modèle sur les données de test en utilisant la fonction evaluation().`\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b6e62-b670-4084-b4be-eea7355fcff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 6, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "plt.figure(figsize=(9, 5))\n",
    "ax = plt.gca()\n",
    "plt.axvline(x=np.log10(ridge_cv.alpha_), linestyle='--', color='k')\n",
    "ax.plot(np.log10(alphas), coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('log(alpha)')\n",
    "ax.set_ylabel('coefficients')\n",
    "ax.set_title('Ridge path')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc8eb8f-b57a-47ac-9508-e05ca3a1b596",
   "metadata": {},
   "source": [
    "# **`Régulatisation`**\n",
    "### **Lasso**\n",
    "\n",
    "\n",
    "`Lasso` (Least Absolute Shrinkage and Selection Operator) est une technique de régularisation pour les modèles de régression linéaire, similaire à la régression Ridge. La régularisation Lasso ajoute une pénalité à la somme des valeurs absolues des coefficients, plutôt qu'à la somme des carrés des coefficients.\n",
    "\n",
    "La fonction de coût pour la régression Lasso est :\n",
    "\n",
    "**`J = MSE + alpha * sum(|w|)`**\n",
    "\n",
    "où `MSE` est l'erreur quadratique moyenne de la régression linéaire ordinaire, `alpha` est un paramètre de régularisation, w est le vecteur des coefficients du modèle (y compris l'interception), et `sum(|w|)` est la somme des valeurs absolues des coefficients.\n",
    "\n",
    "Contrairement à la régression Ridge, la régularisation Lasso peut entraîner des coefficients exactement égaux à zéro. Cela signifie que la régression Lasso peut également être utilisée comme méthode de sélection de variables, car les coefficients des variables non pertinentes peuvent être réduits à zéro. Cependant, cela peut également rendre le modèle plus instable et sensible aux variations aléatoires dans les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab822942-41d6-4626-abd1-d0d351e0afd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Le paramètre **`alpha`** en Lasso contrôle le niveau de régularisation appliqué au modèle. Plus la valeur de alpha est grande, plus la régularisation est forte et plus le modèle sera simple. Cependant, si alpha est trop grand, le modèle peut sous-apprendre et les prédictions seront de mauvaise qualité.\n",
    "\n",
    "- Pour trouver la meilleure valeur de alpha pour un modèle de régression Lasso, on peut utiliser une méthode de recherche par validation croisée. Cette approche consiste à diviser les données en plusieurs ensembles de formation et de test, à entraîner le modèle sur les données de formation avec différentes valeurs de alpha, puis à évaluer les performances du modèle sur les données de test. On sélectionne ensuite la valeur de alpha qui donne les meilleures performances en termes de score de validation croisée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecafa9-4d11-4eed-8e03-4ba7d9ba07ce",
   "metadata": {},
   "source": [
    "# \n",
    "- La classe LassoCV de scikit-learn effectue une recherche par validation croisée pour trouver la meilleure valeur de alpha. L'argument cv spécifie le nombre de plis dans la validation croisée (5 dans cet exemple). L'objet lasso_cv est entraîné sur les données d'entraînement (X_train et y_train), et la meilleure valeur de alpha est stockée dans l'attribut alpha_ de l'objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4701b4-cd2c-4580-8352-6cd7d83f1694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(cv=15)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(\"Meilleure valeur de alpha :\", lasso_cv.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a76b7-275b-4b18-8057-d7f34f894fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = lasso_cv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_model = lasso.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, lasso_model)\n",
    "new_row = {\"Model\": \"lasso_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551957be-7bd8-4464-803d-9d3dd25e8ec7",
   "metadata": {},
   "source": [
    "* `Dans cet exemple,nous avons  créé une instance de la classe Lasso et entraîné le modèle sur les données à l'aide de la méthode fit(). Enfin, nous avons affiché les coefficients de la régression à l'aide des attributs coef_ et intercept_.`\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4937307-2ac3-4f72-a576-1e840f416979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Affichage des coefficients et le biais du modele\n",
    "Coeff_intercept_model(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d03f6-c5c1-4c53-ae0e-e3c7fc100b82",
   "metadata": {},
   "source": [
    "### **Interpretation**\n",
    "*`On peut observer que plusieurs coefficients de régression ont été réduits à zéro pour les variables. Cela suggère que les variable qui'ont conservé un coefficient > ou <  à 0  sont les plus importantes pour la prédiction de y, tandis que les autres variables ont été éliminées par Lasso.`*\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a7983-b7cb-4fb8-b78c-b4194aa2bf8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-6, 6, 100)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    lasso = Lasso(alpha=a, max_iter=10000)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "ax = plt.gca()\n",
    "\n",
    "plt.axvline(x=np.log10(lasso_cv.alpha_), linestyle='--', color='k')\n",
    "\n",
    "ax.plot(np.log10(alphas), coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('log(alpha)')\n",
    "ax.set_ylabel('coefficients')\n",
    "ax.set_title('Lasso path')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6f913-ece3-48f9-898a-da34f0021bf1",
   "metadata": {},
   "source": [
    "- Ce graphique nous permet de suivre l'évolution des coefficients en fonction de la valeur de alpha, représentée sur l'axe horizontal en échelle logarithmique. La ligne verticale en pointillés représente la valeur optimale de alpha qui a été choisie pour le modèle. En comparant les coefficients avant et après l'application de Ridge, nous pouvons observer quels coefficients ont été réduits en valeur grâce à la régularisation.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46683bf-c562-4b12-90a7-22edbb79ffe2",
   "metadata": {},
   "source": [
    "# \n",
    "# **SVR (Support Vector Regression)**\n",
    "\n",
    "`SVR` (Support Vector Regression) est une technique de régression qui utilise la méthode des machines à vecteurs de support (SVM) pour prédire les valeurs continues. Comme pour la classification SVM, l'idée de base de la SVR est de trouver un hyperplan qui maximise la marge entre les points de données et l'hyperplan.\n",
    "\n",
    "Contrairement à la régression linéaire, la SVR peut utiliser des fonctions de noyau non linéaires pour transformer les données en un espace de grande dimension, où l'hyperplan de décision peut être trouvé plus facilement. En général, les fonctions de noyau les plus couramment utilisées sont le noyau linéaire, le noyau polynomial et le noyau RBF (fonction de base radiale).\n",
    "\n",
    "Le principe de base de la SVR est de minimiser l'erreur de prédiction tout en maximisant la marge autour de l'hyperplan de décision. L'erreur de prédiction est mesurée comme la différence entre la valeur prédite et la vraie valeur de la cible. La marge autour de l'hyperplan de décision est définie comme la distance entre l'hyperplan et le point de données le plus proche de l'hyperplan dans l'espace de grande dimension.\n",
    "\n",
    "La fonction de coût pour la SVR est :\n",
    "\n",
    "**`J = C * sum(hinge loss) + 1/2 * ||w||^2`**\n",
    "\n",
    "\n",
    "où `C` est un paramètre de régularisation, hinge loss est la fonction de perte qui mesure l'erreur de prédiction, `w` est le vecteur des coefficients du modèle, et `||w||^2` est la norme L2 des coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d906671-f617-4121-bc92-fec693567a0d",
   "metadata": {},
   "source": [
    "- Pour trouver les meilleures valeurs pour les hyperparamètres d'un modèle de régression à vecteurs de support (SVR), nous allons utiliser une recherche par grille (grid search) ou une recherche aléatoire (random search) sur une plage de valeurs pour chaque hyperparamètre. Les deux approches sont des méthodes de recherche d'hyperparamètres courantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f17e1-dd1a-4692-b2c6-0888225d26bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {'C': uniform(0, 100000), 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "svr = SVR()\n",
    "\n",
    "random_search = RandomizedSearchCV(svr, param_distributions=param_dist, cv=5, n_iter=50, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres : {}\".format(random_search.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf93f3-a226-4d85-a870-b0554895b0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svr = SVR(C= 95732.66861334073, gamma='auto', kernel= 'rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_model = svr.predict(X_test)\n",
    "\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, svr_model)\n",
    "new_row = {\"Model\": \"svr_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92a9d4-8bac-4e68-a2c6-f04470750457",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Remarque**\n",
    "*La valeur du paramètre de régularisation C pour SVM dépend de la complexité de  l'ensemble de données et de la tolérance à l'erreur de prédiction. En général, une valeur plus grande de C donnera un modèle avec une marge d'erreur plus petite, ce qui peut conduire à un surapprentissage si la valeur de C est trop grande.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac02bd-13ec-4311-8ccc-1ba1728c2339",
   "metadata": {},
   "source": [
    "### **Recapitulation**\n",
    "\n",
    "\n",
    "- *`Dans cet exemple, nous avons défini les hyperparamètres à tester à l'aide d'un dictionnaire param_dist. Nous avons  créé un objet de modèle SVR et utilisé la méthode RandomizedSearchCV pour rechercher aléatoirement les meilleures valeurs d'hyperparamètres. Nous avons ajusté le modèle sur les données d'entraînement et utilisé les meilleurs hyperparamètres pour prédire les valeurs sur les données de test.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402c91f-c56b-46db-a69c-fcfa30497c87",
   "metadata": {},
   "source": [
    "#\n",
    "# **DecisionTreeRegressor**\n",
    "\n",
    "- `DecisionTreeRegressor` est une classe de la bibliothèque Python scikit-learn qui permet de construire des modèles de régression à partir d'arbres de décision. Un arbre de décision est une méthode d'apprentissage supervisé qui utilise une série de règles de décision pour prédire les valeurs de la cible.\n",
    "\n",
    "- Le principe de base de l'arbre de décision est de diviser récursivement les données en sous-ensembles plus petits en fonction des valeurs des variables prédictives, jusqu'à ce que chaque sous-ensemble soit suffisamment homogène en termes de valeurs de la cible ou jusqu'à ce qu'une limite de profondeur maximale soit atteinte. Chaque division est choisie de manière à minimiser la variance des valeurs de la cible dans chaque sous-ensemble. L'arbre résultant peut être considéré comme un ensemble de règles de décision simples qui peuvent être facilement interprétées.\n",
    "\n",
    "- L'algorithme de construction de l'arbre de décision pour la régression utilise la mesure de la variance de la cible dans chaque sous-ensemble pour évaluer la qualité de la division. La variance est une mesure de la dispersion des valeurs de la cible dans chaque sous-ensemble, et plus elle est faible, plus les valeurs de la cible sont homogènes. L'algorithme cherche donc à diviser les données de manière à minimiser la variance dans chaque sous-ensemble.\n",
    "\n",
    "- La fonction de coût pour la construction de l'arbre de décision est la variance réduite (reduced variance), qui est définie comme la différence entre la variance totale des valeurs de la cible et la somme pondérée des variances des valeurs de la cible dans chaque sous-ensemble. Plus cette valeur est élevée, meilleure est la division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907e70d-9a84-48e2-8717-ed5e242f347e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "arbre_decision = DecisionTreeRegressor()\n",
    "arbre_decision.fit(X_train, y_train)\n",
    "arbre_decision_model = arbre_decision.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, arbre_decision_model)\n",
    "new_row = {\"Model\": \"arbre_decision_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceece7d-f41c-4646-a17d-49d86a82e229",
   "metadata": {},
   "source": [
    "## \n",
    "- *`Nous avons créé une instance de la classe DecisionTreeRegressor. Nous avons entraîné le modèle sur les données à l'aide de la méthode fit(), puis nous avons utilisé le modèle pour faire des prédictions sur une grille de points de test X_test.`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5fdcc-c6dc-43e9-a72f-05c09274e57d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "\n",
    "`plot_tree` est une fonction de la bibliothèque Python scikit-learn qui permet de visualiser l'arbre de décision construit par un modèle de régression ou de classification. Cette fonction est très utile pour comprendre comment l'arbre de décision prend des décisions et pour interpréter les résultats d'un modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69129f0-157f-41ff-9283-686dda0021bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_text, plot_tree\n",
    "\n",
    "plot_tree(arbre_decision, feature_names=list(X_train.columns),  class_names=y.unique(), filled=True); #Pour afficher le text , supprimer le point virgule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6acbb0-f248-4ea3-92af-130fd9b7d39a",
   "metadata": {},
   "source": [
    "# \n",
    "# **RandomForestRegressor**\n",
    "\n",
    "- `RandomForestRegressor` est un modèle d'apprentissage supervisé de la bibliothèque Python **`scikit-learn`**, qui utilise un ensemble d'**`arbres de décision`** pour effectuer une régression sur des données d'entrée. Le modèle est basé sur la technique d'ensemble learning appelée forêt aléatoire (random forest), qui consiste à entraîner plusieurs arbres de décision sur des sous-ensembles différents des données d'entraînement, puis à combiner les prédictions des arbres pour obtenir une prédiction finale plus robuste.\n",
    "\n",
    "- Le modèle RandomForestRegressor est très utile pour les problèmes de régression non linéaires, car il peut modéliser des relations complexes entre les variables prédictives et la variable cible. Il est également résistant au **surapprentissage (overfitting)** grâce à la technique de bagging (bootstrap aggregating) qui est utilisée pour entraîner les arbres de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2dd80e-aafc-4e04-9cbe-499bcaab83ad",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "**une recherche aléatoire (randomized search) des meilleurs hyperparamètres pour un modèle RandomForestRegressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066568d-ae1b-4e1d-8938-300da01601c4",
   "metadata": {},
   "source": [
    "**`RandomizedSearchCV`** est une fonction de la bibliothèque Python scikit-learn qui permet d'effectuer une recherche aléatoire (randomized search) des meilleurs hyperparamètres pour un modèle d'apprentissage supervisé en utilisant une validation croisée (cross-validation). La recherche aléatoire des hyperparamètres est utile lorsque l'espace des hyperparamètres est grand et qu'il est difficile ou coûteux de l'explorer entièrement avec une recherche exhaustive.\n",
    "\n",
    "La fonction RandomizedSearchCV prend en entrée un modèle d'apprentissage supervisé, une grille d'hyperparamètres à explorer, un nombre de combinaisons aléatoires à tester, une métrique d'évaluation et une stratégie de validation croisée. Elle retourne le meilleur ensemble d'hyperparamètres trouvés et la performance associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d5076-793a-4974-ba6d-c465035764ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'max_features': max_features\n",
    "}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2c339-5bdb-4021-a116-711244ed7277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=500, min_samples_split= 2, min_samples_leaf=2, max_features='auto', max_depth= 80)\n",
    "random_forest.fit(X_train, y_train)\n",
    "foret_aleatoire_model = random_forest.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, foret_aleatoire_model)\n",
    "new_row = {\"Model\": \"foret_aleatoire_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974f5e5-2869-4286-9787-e4fea5049f30",
   "metadata": {},
   "source": [
    "### **Recapitulation**\n",
    "\n",
    "- *`Dans cet exemple, nous avons défini une grille d'hyperparamètres à explorer pour le modèle RandomForestRegressor, comprenant le nombre d'arbres (n_estimators), le nombre maximum de variables à considérer à chaque division (max_features), la profondeur maximale des arbres (max_depth), le nombre minimum d'échantillons requis pour une division (min_samples_split) et le nombre minimum d'échantillons requis pour une feuille (min_samples_leaf). Nous avons créé un objet RandomizedSearchCV en spécifiant le modèle, la grille d'hyperparamètres, la stratégie de validation croisée (cv),une graine aléatoire (random_state). Nous avons lancé la recherche aléatoire des hyperparamètres sur les données d'entraînement.`*\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3b601-183d-44dd-bcf6-ac960d91cfdb",
   "metadata": {},
   "source": [
    "# \n",
    "# MLPRegressor\n",
    "\n",
    "- Le modèle de **`MLPRegressor`** est un réseau de neurones artificiels qui utilise une ou plusieurs couches cachées de neurones interconnectés pour apprendre une fonction non linéaire qui mappe un ensemble de variables d'entrée à une variable de sortie continue. Les couches cachées sont composées de neurones qui appliquent une fonction d'activation non linéaire à une combinaison linéaire des entrées pondérées.\n",
    "\n",
    "- L'objectif de l'algorithme de MLPRegressor est de **minimiser** la `fonction de perte` (loss function) qui mesure l'écart entre les prédictions du modèle et les vraies valeurs de sortie. Pour cela, il utilise la technique de rétropropagation du gradient (backpropagation) pour ajuster les poids des connexions entre les neurones et améliorer les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0138174-357a-4bdd-a772-8e12fcf1282c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "MLPRegressor = MLPRegressor(hidden_layer_sizes=15,solver='lbfgs',max_iter=1500,random_state=80)\n",
    "\n",
    "MLPRegressor.fit(X_train, y_train)\n",
    "MLPRegressor_model = MLPRegressor.predict(X_test)\n",
    "\n",
    "mae, mse, rmse, r_squared = evaluation(y_test, MLPRegressor_model)\n",
    "new_row = {\"Model\": \"MLPRegressor_model\",\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2 Score\": r_squared}\n",
    "models = models.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74ec99-3825-4723-9fd6-17c88eb148eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, MLPRegressor_model)\n",
    "plt.plot([y_test.min(), y_test.max()], [MLPRegressor_model.min(), MLPRegressor_model.max()], '--', color='red')\n",
    "plt.xlabel('Valeurs réelles')\n",
    "plt.ylabel('Prédiction')\n",
    "plt.title('Prédiction vs. Valeurs réelles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7c725-fb33-4c08-afc6-18efe8eb3ee4",
   "metadata": {},
   "source": [
    "- Le graphe montre  une ligne en pointillés rouges représentant la diagonale, qui correspondrait à une prédiction parfaite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3de25-8c21-4f06-a2e5-d0065362ad55",
   "metadata": {},
   "source": [
    "### Recapitulation\n",
    "- *`Dans cet exemple,nous avons créé un objet MLPRegressor en spécifiant le nombre et la taille des couches cachées (hidden_layer_sizes), l'algorithme d'optimisation (solver), le nombre maximum d'itérations (max_iter) et une graine aléatoire (random_state). Nous avons entraîné le modèle sur les données d'entraînement à l'aide de la méthode fit, et prédit les valeurs de sortie pour les données de test à l'aide de la méthode predict. Enfin, nous avons calculé la performance du modèle en utilisant la fonction evaluation .`*\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf95ecc-7b6a-44c4-b0c7-0cf4d8687a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models.sort_values(by=\"R2 Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401e558-cd60-46b0-9b9c-682c2faee280",
   "metadata": {},
   "source": [
    "- La ligne de code \"models.sort_values(by='R2 Score')\" trie les lignes du DataFrame \"models\" en fonction des valeurs de la colonne \"R2 Score\" (le coefficient de détermination). Elle renvoie un nouveau DataFrame trié dans l'ordre croissant des valeurs de \"R2 Score\". Cette ligne de code est souvent utilisée pour classer les modèles selon leur performance sur une métrique particulière, dans ce cas, le R2 Score.\n",
    "\n",
    "- Cela permet de visualiser rapidement quel modèle a la meilleure performance sur cette métrique, en le plaçant en haut de la liste.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9007616-afcb-48d2-bec7-dc60b7729113",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "### **Vous pouvez visualiser les resultats des modeles pour plusieurs `metriques`, il suffit de fixer l'axe **y** par la metrique qui vous convient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0dbf9-5d15-41d4-8753-dbccea1e2480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x=models[\"Model\"], y=models[\"R2 Score\"])\n",
    "plt.title(\"Models' RMSE Scores (Cross-Validated)\", size=10)\n",
    "plt.xticks(rotation=90, size=10)\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ed947-0bb4-4c95-b9ed-3e8d51492fe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \n",
    "# **Conclusion**\n",
    "*`En conclusion, les modèles de régression linéaire sont des méthodes puissantes de modélisation statistique qui permettent d'analyser et de prédire les relations entre des variables quantitatives. Les modèles de régression linéaire simples sont relativement faciles à comprendre et à interpréter, tandis que les modèles de régression linéaire multiple peuvent être utilisés pour étudier les interactions entre plusieurs variables indépendantes.`*\n",
    "\n",
    "*`Il existe plusieurs techniques pour entraîner des modèles de régression linéaire, chacune avec ses avantages et ses inconvénients. Les modèles de régression linéaire régularisée, tels que la régression Ridge et la régression Lasso, permettent de régulariser les coefficients de régression et de réduire la variance du modèle, ce qui peut améliorer les performances de prédiction.`*\n",
    "\n",
    "*`Cependant, il est important de se rappeler que les modèles de régression linéaire ont des hypothèses strictes, notamment en ce qui concerne la linéarité, l'indépendance, l'homoscédasticité et la normalité des résidus. Il est donc important de vérifier que ces hypothèses sont satisfaites avant d'interpréter les résultats du modèle.`*\n",
    "\n",
    "*`Enfin, il est souvent utile d'évaluer les performances des modèles de régression linéaire en utilisant une combinaison de métriques d'erreur, telles que le MSE, le RMSE, le MAE ou le R². Il est également important d'utiliser des techniques de validation croisée pour éviter le surapprentissage et de régulariser les modèles si nécessaire.`*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
